{
  "hash": "1ef28e8bf4872640e1251fb70eabce72",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Deploying a computer vision model using Fastai, Gradio and Hugging Face Spaces'\ndescription: Learn to train and deploy your own computer vision model.\nauthor: Stan Karzhev\ndate: '2024-03-28'\nimage: cover2.png\ntoc: true\ncategories:\n  - Deep Learning\n---\n\n# Introduction\nTraining a model can often be the most exciting part of deep learning, but overall it's a subsection of the broader data science pipeline. This includes vital steps such as data engineering and model deployment. Not to mention that once a model is trained and validated model, it's typically within in Jupyter Notebook, a Python file or a `.pkl` (Pickle) file. These are all static environments and only allow for limited user interaction if any.\n\nIn this guide, we will focus on fine-tuning a pre-trained CNN model for classifying different interior design styles, creating a demo app using [Gradio](https://github.com/gradio-app/gradio) (an open-source Python library that makes it quick and easy to create ML demos) and hosting the model live on [HuggingFace Spaces](https://huggingface.co/spaces). \n\n\n# Creating a Dataset\nWe will begin by creating a dataset necessary for fine-tuning our model, which is crucial for it to learn the distinctive features of various interior styles. In this case, we use the DuckDuckGoSearch API, mainly for its simplicity and efficiency in gathering images. These images represent the interior styles: Bohemian, Bauhaus and Scandinavian, which will be downloaded and organised into separate directories. This approach facilitates the model's learning process, allowing it to adapt to the nuances of each interior design style.\n```python\nfrom itertools import islice\nfrom duckduckgo_search import DDGS\nfrom fastcore.all import *\n\ndef search_images(term, max_images=50):\n    print(f\"Searching for {term}\")\n    # DDGS().images returns an iterator containing all images found\n    # islice is used to limit the number of results returned\n    return L(islice(DDGS().images(term), max_images)).itemgot('image')\n```\n\n```python\nsearches = [\"bohemian\", \"scandinavian\", \"bauhaus\"]\npath = Path('interiors')\nfrom time import sleep\n\nfor o in searches:\n    dest = path/o\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f\"{o} interior design\", max_images=500))\n    sleep(10)\n    resize_images(path/o, max_size=256, dest=path/o)\n```\n\nIn the data preparation phase, we utilise the Fastai `DataBlock` class to structure our data appropriately for model training. This process involves several key steps: \n\n```python\ndls = DataBlock(\n    blocks = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    splitter = RandomSplitter(valid_pct=0.2, seed=44),\n    get_y = parent_label,\n    item_tfms=RandomResizedCrop(size=(256), min_scale=0.8),# Apply RandomResizedCrop as an item level transformation\n    batch_tfms = aug_transforms(max_rotate=5, p_lighting=0.8) # Apply standard augmentations at the batch level\n    ).dataloaders(path, bs=64)\n\ndls.show_batch(max_n=8)\n```\n\n- **Data Types**: We specify the types of data we're dealing with, `ImageBlock` for the input images and `CategoryBlock` for the targets, which in this are the different interior styles.\n- **Image Collection**: The `get_image_files` function recursively collects all image files from the specified directory, ensuring our model has access to the entire dataset for training and validation.\n- **Data Splitting**: We use the `RandomSplitter` function to allocate 20% of the data to a validation set. This random split, seeded for reproducibility, helps in evaluating the model's performance on unseen data.\n- **Labelling**: The `get_y` function specifies how to extract the category labels. Here we use `parent_label` which gets the labels from parent directory names, where each class has its own folder.\n- **Transformations**:\n    - **Item-level**: The `RandomResizedCrop` transformation randomly crops the images, retaining 80% of the original image and resizing to 256x256 pixels. This introduces variability in the training data, ensuring the model is more robust to different sizes and orientations.\n    - **Batch-level**: `aug_transforms` applies more generalised augmentations, such as slight rotations (up to 5 degrees) and lighting adjustments (probability of changing brightness and contrast), enhancing the model's ability to generalise from the training data by exposing it to a wider variety of visual conditions. \n\n![Example of transformations applied to our data](batch.png)\n\n# Model Training\nIn the model training phase, leveraging a pre-trained model such as ResNet50, known for it's depth and robustness, can significantly speed up training and enhance performance. ResNet50 is a Convolutional Neural Network (CNN) with 50 layers and has been trained on ImageNet a comprehensive image dataset containing 1.2 million images across 1000 categories. This extensive pre-training enables ResNet50 to recognise a broad array of features, making it a powerful starting point for our task. \n\nIn Fastai the `vision_learner` function is used to create a `Learner` object for training image models. This function is part of the high-level API that simplifies the process of model instantiation and training. Here's how we do it:\n\n```python\nlearn = vision_learner(dls, resnet50, metrics=error_rate)\n\n```\n\n- **dls**: The data loaders created earlier, which contain our structured image data.\n- **resnet50**: Specifies the architecture of the pre-trained model we're using.\n- **metrics**: We use `error_rate` to evaluate model performance, indicating the proportion of incorrect predictions.\n\nNext, we fine-tune the model:\n\n```python\nlearn.fine_tune(3)\n```\n\nThe `vision_learner` function automatically modifies the ResNet50 model to suit our specific classification task. It does this by replacing the head of the network, originally designed for the original classes in the ImageNet dataset, with a new head tailored to classify the distinct interior styles in our dataset.\n\n\n:::::: {.callout-note}\nThe \"head\" of a model, in the context of transfer learning, refers to the part of the network that is specifically adjusted or replaced to make the model suitable for the target task. It includes the final few layers responsible for making the final predictions.\n:::\n\nThe `fine_tune` method in Fastai occurs in two steps:\n\n1. In the first phase, the model trains only on the newly added head on the target dataset while keeping the rest of the model \"frozen\", i.e. the parameters of that part of the neural network are not updated during training.\n2. In the second phase, the entire model is unfrozen allowing, all the layers to be trained further. However, not all layers are trained equally, fastai applies differential learning rates, meaning earlier layers usually have smaller learning rates compared to the newly added head. This approach helps in fine-tuning the pre-trained weights without distorting them too much, while preserving the previously learned features, helping to avoid catastrophic forgetting.\n\n:::::: {.callout-note}\nCatastrophic forgetting occurs when an artificial neural network “forgets” features that it had previously learned upon learning new information.\n:::\n\nAfter training for 3 epochs, we can achieve ~97% accuracy on our interior image dataset, which took only a few seconds to train.\n\n\n![](train.png)\n\nThe next step is to export our model and save it in a `.pkl` file.\n\n```python\nlearn.export(\"model.pkl\")\n```\n\n# Creating a Demo with Gradio\nTo create a demo for our model with Gradio, we must first load our saved model.\n\n```python\nlearn = load_learner('model.pkl')\n```\n\nThe next thing we need is a predict function, essentially this will act as the bridge between Gradio and our Fastai model. Gradio requires a function that it can call with an input (an image in this case) to obtain predictions. The `predict` function wraps around the `learn.predict` method, which is what we use to make predictions using our trained model. It processes an input image, feeds it into the model, and then processes the model's output into a more readable format. \n\n```python\n# Define the prediction function\ndef predict(img):\n    try:\n        # Create the image object\n        img = PILImage.create(img)\n        # Get predictions from the model\n        pred, pred_idx, probs = learn.predict(img)\n        # Fetch the labels dynamically from the model's vocabulary\n        labels = learn.dls.vocab\n        # Ensure probabilities are floats\n        return {labels[i]: float(probs[i]) for i in range(len(labels))}\n```\n\nGradio will display the output of the predict function in the web interface, by converting the predictions into a dictionary where each label is associated with its probability, making the output more interpretable for an end user. \n\n:::::: {.callout-note}\nMake sure you have the Gradio library installed first, using `pip install gradio`\n:::\n\n```python\n# Set up Gradio interface\ninterface = gr.Interface(\n    fn=predict,\n    inputs=gr.Image(type=\"pil\"),\n    outputs=gr.Label(num_top_classes=3),\n)\n```\n\nOnce we have setup the interface, we can go ahead and launch it:\n\n```python\ninterface.launch(share=True, debug=False)\n```\n\n![You should see an interface similar to this.](gradio.png)\n\nWe can specify further arguments to the Gradio interface object which allows for more customiseability. For instance:\n\n```python\ntitle = \"Interior Design Classifier\"\ndescription = \"Upload an image of an interior design and get a prediction of the design style.\"\nexamples = ['1.jpeg', '2.jpg', '3.jpg']\n\ninterface = gr.Interface(\n    fn=predict,\n    inputs=gr.Image(type=\"pil\"),\n    outputs=gr.Label(num_top_classes=3),\n    title=title,\n    description=description,\n    examples=examples\n)\n```\n\nHere we are providing a title and a description along with a few example images that users can simply click on in the interface to get classification results. \n\n:::::: {.callout-note}\nAny example files we are using for our interface need to also be uploaded within our HF space.\n:::\n\n# Model deployment on HF spaces\n\nTo deploy our model on HuggingFace spaces, we'll have to create:\n\n- A HuggingFace account.\n- An `app.py` file containing our model import, `predict` function and gradio interface code.\n- A `requirements.txt` which will list the libraries we are using on separate lines (fastai, gradio and Pillow).\n- Our `model.pkl`.\n\nWe will also need to create a new HuggingFace \"space\" which is similar to a Git repository. This is where we will upload all our files.\n\n![Configuration settings for a new HuggingFace space](HF_space.png)\n\n::::: {.callout-note}\nOur model file will likely be too large to upload via HuggingFace spaces, so we will need to either upload via the HF UI or utilise [Git LFS](https://git-lfs.com/).\n:::\n\nOnce you've added all the files, you should see your model app load within your HuggingFace spaces in a few minutes.\n\n![](demo.png)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}