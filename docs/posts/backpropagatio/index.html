<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Stan Karzhev">
<meta name="dcterms.date" content="2024-01-31">
<meta name="description" content="….">

<title>Stan Karzhev - Backpropagation: The Essence of Neural Network Training</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Stan Karzhev</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/StanKarz" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/stanislav-karzhev-587808232/" target="_blank"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Backpropagation: The Essence of Neural Network Training</h1>
                  <div>
        <div class="description">
          ….
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Deep Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Stan Karzhev </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 31, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#what-is-backpropagation" id="toc-what-is-backpropagation" class="nav-link active" data-scroll-target="#what-is-backpropagation">What is Backpropagation</a></li>
  <li><a href="#artificial-neural-networks" id="toc-artificial-neural-networks" class="nav-link" data-scroll-target="#artificial-neural-networks">Artificial Neural Networks</a>
  <ul class="collapse">
  <li><a href="#understanding-the-neural-network-architecture" id="toc-understanding-the-neural-network-architecture" class="nav-link" data-scroll-target="#understanding-the-neural-network-architecture">Understanding the Neural Network Architecture</a></li>
  <li><a href="#from-inputs-to-hidden-neurons" id="toc-from-inputs-to-hidden-neurons" class="nav-link" data-scroll-target="#from-inputs-to-hidden-neurons">From Inputs to Hidden Neurons</a></li>
  <li><a href="#diving-deeper-into-activation-functions" id="toc-diving-deeper-into-activation-functions" class="nav-link" data-scroll-target="#diving-deeper-into-activation-functions">Diving Deeper into Activation Functions</a></li>
  <li><a href="#demonstrating-a-forward-pass" id="toc-demonstrating-a-forward-pass" class="nav-link" data-scroll-target="#demonstrating-a-forward-pass">Demonstrating a Forward Pass</a></li>
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function">Loss Function</a></li>
  <li><a href="#other-loss-functions" id="toc-other-loss-functions" class="nav-link" data-scroll-target="#other-loss-functions">Other Loss Functions</a></li>
  </ul></li>
  <li><a href="#learning-by-reversing" id="toc-learning-by-reversing" class="nav-link" data-scroll-target="#learning-by-reversing">Learning by Reversing</a>
  <ul class="collapse">
  <li><a href="#the-chain-rule" id="toc-the-chain-rule" class="nav-link" data-scroll-target="#the-chain-rule">The Chain Rule</a></li>
  <li><a href="#computing-gradients" id="toc-computing-gradients" class="nav-link" data-scroll-target="#computing-gradients">Computing Gradients</a></li>
  <li><a href="#updating-parameters" id="toc-updating-parameters" class="nav-link" data-scroll-target="#updating-parameters"><strong>Updating Parameters</strong></a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="what-is-backpropagation" class="level1">
<h1>What is Backpropagation</h1>
<p>Imagine teaching a robot to distinguish between cats and other animals, each time it makes a mistake you adjust its “thought process” slightly, to make it better at this task. In artificial intelligence, this fine-tuning is achieved through an algorithm known as backpropagation. It’s a method that iteratively adjusts a neural network’s parameters, steering it towards higher accuracy.</p>
<p>But how does backpropagation know which way to adjust? It uses calculus, specifically by finding the gradient with respect to a loss function. This gradient acts like a compass, pointing towards the direction where the network’s output or predictions become more accurate. This process is not just limited to neural networks; it’s a general technique applicable to various mathematical expressions, making it a universal tool in machine learning.</p>
<p>Since its inception, backpropagation has been pivotal in the resurgence and success of neural networks, marking a key milestone in the AI revolution. In the upcoming sections, we’ll explore the building blocks of neural networks, the intuition and math behind backpropagation and how it’s applied to train neural networks.</p>
</section>
<section id="artificial-neural-networks" class="level1">
<h1>Artificial Neural Networks</h1>
<p>Artificial neural networks (ANNs) are designed to emulate the way biological neurons in the human brain process information. These networks comprise interconnected nodes or neurons arranged in layers, communicating through connections akin to biological synapses. Each neuron in an ANN receives and processes data, contributing to the overall task of the network, such as classification or pattern recognition. By mimicking these biological processes, ANNs harness the complex, adaptive nature of the human brain to solve diverse computational problems.</p>
<section id="understanding-the-neural-network-architecture" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-neural-network-architecture">Understanding the Neural Network Architecture</h3>
<p>[Diagram here]</p>
<p>Before going into the details of backpropagation, it’s essential to grasp how neural nets are structured and the concept of forward propagation. Consider a simple neural network that consists of a few layers:</p>
<ul>
<li><strong>Input Layer:</strong> This is where the network takes in data. The nodes <code>x1</code> and <code>x2</code> represent the features of this input data. A bias unit is often included in this layer to account for offset in the decision boundary.</li>
<li><strong>Hidden Layer:</strong> In this layer, the input data is processed by the neurons <code>a1</code>, <code>a2</code> and <code>a3</code>. Each connection from the input to the hidden layer is weighted <span class="math inline">\(W_{ij}\)</span> , the strength of these weights signifies the importance of the input values to the neurons.</li>
<li><strong>Activation Function:</strong> Each neuron in the hidden layer processes input weighted data by summing them and applying an activation function. This aspect is crucial as these functions introduce non-linearity in the network and enable the network to capture complex patterns. Without them, the neural network would become a linear regressor, regardless of how many layers we add.</li>
<li><strong>Output Layer:</strong> The neuron <code>y1</code> receives processed information from the hidden layer neurons and through weighted connections <span class="math inline">\(M_{1j}\)</span>, generates the final output after applying an activation function.</li>
</ul>
</section>
<section id="from-inputs-to-hidden-neurons" class="level3">
<h3 class="anchored" data-anchor-id="from-inputs-to-hidden-neurons">From Inputs to Hidden Neurons</h3>
<p>Now that we are familiar with the structure of a neural net we can perform forward propagation to obtain the output. Each neuron in the hidden layer calculates a weighted sum of its inputs and a bias term. This sum, known as the neuron’s activation potential, is then transformed by an activation function, which determines the neuron’s output. For neuron a1, the activation potential is calculated as follows:</p>
<p><span class="math inline">\(activation_1 = x1 \cdot + x2 \cdot w12 + w10\)</span></p>
<p>This result is then passed through an activation function to determine the neuron’s output.</p>
<p><span class="math inline">\(a_1 = f(activation_1)\)</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Weights can be positive (excitatory, amplifying the signal) or negative (inhibitory, reducing the signal), affecting the neuron’s activation.</p>
</div>
</div>
</section>
<section id="diving-deeper-into-activation-functions" class="level3">
<h3 class="anchored" data-anchor-id="diving-deeper-into-activation-functions">Diving Deeper into Activation Functions</h3>
<p>While we’ve previously mentioned how activation functions introduce non-linearity, let’s explore specific use cases and characteristics of common functions such as sigmoid, Tanh and ReLU.</p>
<ul>
<li><p><strong>Sigmoid:</strong> The sigmoid function’s smooth S-shaped curve allows it to approximate binary functions closely. It is continuous and differentiable, which makes it suitable for gradient-based optimisation methods. However, it can result in <strong>vanishing gradients</strong>, where gradients shrink and their derivates become tiny, resulting in negligible parameter updates and effectively halting the network from further training. This is especially problematic in deep networks with many layers.</p></li>
<li><p><strong>Tanh:</strong> Often used in hidden layers where data is zero-centered; it helps in transitioning data effectively through layers without shifting the mean, which can accelerate learning. However, Tanh also suffers from <strong>vanishing gradients</strong>, although to a lesser extent than the sigmoid due to its symmetric output range. This range can help with the convergence in training because it tends to centre the output of the neurons. Neurons in later layers of deep networks start with activations that are not too small or too large, which can make learning more stable in the early stages.</p></li>
<li><p><strong>ReLU:</strong> ReLU, short for Rectified Linear Unit, is widely used in deep neural networks due to its ability to facilitate faster and more effective learning, particularly in layers deep within the network. It directly counters the vanishing gradient problem by allowing positive gradients to flow through without change, which accelerates convergence during training. Despite its advantages, ReLU can be susceptible to <strong>“neuron death”</strong> during training, where neurons may stop participating in the data transformation process if their gradients reach zero, known as the <strong>“dying ReLU”</strong> issue.</p></li>
</ul>
<p>[diagram here]</p>
</section>
<section id="demonstrating-a-forward-pass" class="level3">
<h3 class="anchored" data-anchor-id="demonstrating-a-forward-pass">Demonstrating a Forward Pass</h3>
<p>Going back to our original diagram and using the concepts covered so far, namely layers, weights, biases and activation functions we will now complete a forward pass to obtain the output of our neural network using the inputs, network parameters and sigmoid activation function.</p>
<ul>
<li><strong>Inputs:</strong> <span class="math inline">\(x1 = 1, x2 = -1\)</span></li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(w11\)</span></td>
<td>1</td>
</tr>
<tr class="even">
<td><span class="math inline">\(w21\)</span></td>
<td>0.5</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(w12\)</span></td>
<td>-0.5</td>
</tr>
<tr class="even">
<td><span class="math inline">\(w22\)</span></td>
<td>1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(w31\)</span></td>
<td>0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(w32\)</span></td>
<td>0.5</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(m11\)</span></td>
<td>0.25</td>
</tr>
<tr class="even">
<td><span class="math inline">\(m12\)</span></td>
<td>-1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(m13\)</span></td>
<td>1</td>
</tr>
<tr class="even">
<td><span class="math inline">\(w10\)</span></td>
<td>0.10</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(w20\)</span></td>
<td>0.20</td>
</tr>
<tr class="even">
<td><span class="math inline">\(w30\)</span></td>
<td>-0.1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(m10\)</span></td>
<td>1</td>
</tr>
</tbody>
</table>
<ul>
<li><span class="math inline">\(f(z)  \frac 1 {1+e^{-z}}\)</span></li>
</ul>
<p><span class="math inline">\(a_1 = f(x1 \cdot w11 + x2 \cdot w12 +  w10) = 0.83\)</span></p>
<p><span class="math inline">\(a_2 = f(x1 \cdot w21 + x2 \cdot w22 +  w20) =
0.430\)</span></p>
<p><span class="math inline">\(a_3 = f(x1 \cdot w31 + x2 \cdot w32 +  w30) = 0.354\)</span></p>
<p><span class="math inline">\(y_1 = f(a1 \cdot m11 + a2 \cdot m12 + a3 \cdot m13 + m10) = 0.532\)</span></p>
</section>
<section id="loss-function" class="level3">
<h3 class="anchored" data-anchor-id="loss-function">Loss Function</h3>
<p>Now that we know the output from our network we need to evaluate the accuracy of its prediction, a way to quantify how close we are to the true value. This is where the loss function comes into play. The loss function measures the disparity between the predicted output of the neural network and the actual output. By calculating this disparity, we can determine how well the network is performing and use it to guide the backpropagation process for further improvement. For this example, we will use mean squared error as our loss.</p>
<p><span class="math inline">\(\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2\)</span></p>
<p><strong>Where:</strong></p>
<ul>
<li><p><span class="math inline">\(n\)</span> is the number of samples</p></li>
<li><p><span class="math inline">\(Y_i\)</span> is the observed value</p></li>
<li><p><span class="math inline">\(\hat Y_i\)</span></p></li>
</ul>
<p>For instance, if the actual value of y is 1 and we know that our neural network predicted 0.532 then we can find the error associated with this output:</p>
<p><span class="math inline">\(MSE = (1 - 0.532)^2 = 0.219\)</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this case, we consider the number of samples to be 1, but this usually won’t be the case as the error will be averaged over many samples in the training set.</p>
</div>
</div>
</section>
<section id="other-loss-functions" class="level3">
<h3 class="anchored" data-anchor-id="other-loss-functions">Other Loss Functions</h3>
<p>Selecting an appropriate loss function is another important consideration when training a neural network, this usually depends on the type of task and has significant implications for the convergence and performance of the neural network. Here are a few other common loss functions:</p>
<p><strong>Mean Absolute Error (MAE):</strong></p>
<ul>
<li><p><strong>Use Cases:</strong> Regression tasks</p></li>
<li><p><strong>Advantages:</strong> Robust to outliers as errors are not squared in the calculation.</p></li>
<li><p><strong>Disadvantages:</strong> Because the errors are not squared this ****can understate the impact of large errors, which may not always be desirable.</p></li>
<li><p><strong>Mathematical Expression:</strong></p></li>
</ul>
<p><span class="math inline">\(MAE = \frac 1N \sum_{i=1}^{N} |y_i -\hat y|\)</span></p>
<p><strong>Binary Cross Entropy (Log Loss)</strong></p>
<ul>
<li><p><strong>Use Cases:</strong> Binary classification tasks.</p></li>
<li><p><strong>Advantages:</strong> Well suited for measuring the performance of a classification model whose output is a probability value between 0 and 1.</p></li>
<li><p><strong>Disadvantages:</strong> Can be sensitive to imbalanced datasets where the number of instances of a class significantly outnumber the other.</p></li>
<li><p><strong>Mathemtical Expression</strong>: <span class="math inline">\(L_{\text{BCE}} = -\frac{1}{N} \sum_{i=1}^{N} \left [(Y_i \cdot \log\hat{Y}_i + (1 - Y_i) \cdot \log(1 - \hat{Y}_i)) \right]\)</span></p></li>
</ul>
<p><strong>Categorical Cross-Entropy:</strong></p>
<ul>
<li><p><strong>Use Cases:</strong> Multi-class classification tasks.</p></li>
<li><p><strong>Advantages:</strong> Generalises binary cross-entropy to multiple classes and is effective when each sample belongs to exactly one class.</p></li>
<li><p><strong>Disadvantages:</strong> Like binary cross-entropy, it can be sensitive to imbalanced datasets.</p></li>
<li><p><strong>Mathematical Expression:</strong></p></li>
</ul>
<p><span class="math inline">\(L_{CCE} = \frac 1N \sum_{i=1}^{N} \sum_{c=1}^{M}y_{ic}\cdot log(\hat y_{ic})\)</span></p>
<p><strong>Where:</strong> <span class="math inline">\(M\)</span> is the number of classes</p>
<p><strong>Hinge Loss</strong></p>
<ul>
<li><p><strong>Use Cases:</strong> Binary classification tasks, especially for support vector machines (SVMs).</p></li>
<li><p><strong>Advantages:</strong> It encourages the model to not only make the correct prediction but also to make it with high confidence.</p></li>
<li><p><strong>Disadvantages:</strong> Not suitable for probability estimates, as it does not model probability distributions.</p></li>
<li><p><strong>Mathematical Expression:</strong></p></li>
</ul>
<p><span class="math display">\[
L_{Hinge} = \frac 1N \sum_{i=1}^{N} max(0, 1 - y_i \cdot \hat y_i)
\]</span></p>
</section>
</section>
<section id="learning-by-reversing" class="level1">
<h1>Learning by Reversing</h1>
<p>With our neural network’s performance quantified by the loss function, we now face the challenge of improving it. This is where backpropagation comes into play. Backpropagation is a methodical way of adjusting the weights in our network to minimise the loss. By calculating how the loss changes with respect to each weight, we can adjust the weights in the direction that reduces the loss, iteratively enhancing the network’s predictions. By repeatedly applying forward and backpropagation across multiple iterations over the entire dataset, the network ‘learns’ a more precise model of the input-output relationship. Ultimately, this allows the network to incrementally capture the underlying patterns in the data, leading to more accurate predictions</p>
<section id="the-chain-rule" class="level3">
<h3 class="anchored" data-anchor-id="the-chain-rule">The Chain Rule</h3>
<p>The chain rule from calculus is expressed as:</p>
<p><span class="math display">\[
\frac {dy}{dx} = \frac {dy}{du} \cdot \frac {du}{dx}
\]</span></p>
<p>The chain rule helps us understand how two rates of change are related. Imagine <span class="math inline">\(y\)</span> depends on <span class="math inline">\(u\)</span> and <span class="math inline">\(u\)</span> depends on <span class="math inline">\(x\)</span>. If you know how quickly <span class="math inline">\(y\)</span> changes with <span class="math inline">\(u\)</span> and how quickly <span class="math inline">\(u\)</span> changes with <span class="math inline">\(x\)</span>, then the chain rule allows us to multiply these two rates to find out the rate of change of <span class="math inline">\(y\)</span> with respect to <span class="math inline">\(x\)</span>. It’s like figuring out the total effect on <span class="math inline">\(y\)</span> when <span class="math inline">\(x\)</span> changes, by linking their relationship through <span class="math inline">\(u\)</span>.</p>
</section>
<section id="computing-gradients" class="level3">
<h3 class="anchored" data-anchor-id="computing-gradients">Computing Gradients</h3>
<p>With the chain rule in mind, we can calculate the gradient of the loss function with respect to each weight in the neural network. Essentially, backpropagation reverses the flow of computation in the neural network. Starting from the output it propagates the error back through the network, layer by layer. At each neuron, the chain rule is used to find out to what extent weights and biases need to be adjusted to reduce the error, linking the rate of change of the error with respect to the output (which we want to minimise) to the rate of change of the output with respect to the weights and biases (which we can control). This linkage allows us to “backpropagate” the error and update our weights to improve our model.</p>
<p>For instance, applying the chain rule for the weight <span class="math inline">\(m11\)</span> gives us the expression:</p>
<p><span class="math display">\[ \frac{\partial \theta}{\partial m11} = \frac{\partial \theta}{\partial y1} \cdot \frac{\partial y1}{\partial net} \cdot \frac{\partial net}{\partial m11}
\]</span></p>
<p><strong>Where:</strong></p>
<ul>
<li><span class="math inline">\(y1 = f(net)\)</span></li>
<li><span class="math inline">\(net = a1 \cdot m11 + a2 \cdot m12 + a3 \cdot m13 + m10\)</span></li>
<li><span class="math inline">\(\theta\)</span> is the loss function</li>
</ul>
<p>The weight m11 directly influences the output of the neural network by scaling the activation a1 from the hidden layer before it contributes to the output neuron y1. In essence, m11 modulates how much a1 impacts the final prediction. By adjusting m11, we can control the strength of this influence. During the training process, we seek to find the optimal value of m11 that minimises the loss function.</p>
<p>Going further back in the network, we can derive the expression for <span class="math inline">\(w22\)</span> too:</p>
<p><span class="math inline">\(\frac{\partial \theta}{\partial w22} = \frac{\partial \theta}{\partial y1} \cdot \frac{\partial y1}{\partial f(a2)} \cdot \frac{\partial f(a2)}{\partial a2} \cdot \frac{\partial a2}{\partial w22}\)</span></p>
<p>The weight <code>w22</code> plays a pivotal role in determining the output of a neural network by influencing the activation of neuron <code>a2</code> in the hidden layer. A change in <code>w22</code> alters the weighted input to <code>a2</code>, which, due to the sigmoid function, significantly affects <code>a2's</code> activation level. This change is then carried forward, impacting the input and, subsequently, the activation of the output neuron <code>y1</code>. Since the output <code>y1</code> is a result of applying the sigmoid function to its total input, any modification in <code>w22</code> leads to a non-linear alteration in <code>y1</code>. This alteration cascades to affect the network’s loss <code>θ</code>, akin to a domino effect, where adjusting the initial element <code>w22</code>, steers the network towards a lower loss and improved performance.</p>
</section>
<section id="updating-parameters" class="level3">
<h3 class="anchored" data-anchor-id="updating-parameters"><strong>Updating Parameters</strong></h3>
<p>The computed gradients are used to make small adjustments to the weights and biases, a process guided by the learning rate, a hyperparameter that controls the size of these adjustments. This step is crucial; too large a learning rate can lead to overshooting the minimum of the loss function, while too small a learning rate can cause the training process to stall.</p>
<p>Our update equation for <code>w11</code> becomes:</p>
<p><span class="math inline">\(m11 = m11 - \alpha \left[\frac{\partial \theta}{\partial y1} \cdot \frac{\partial y1}{\partial net} \cdot \frac{\partial net}{\partial m11}
\right]\)</span></p>
<p><strong>Where:</strong></p>
<ul>
<li><span class="math inline">\(\alpha\)</span> is the learning rate</li>
</ul>
<p>The partial derivative <span class="math inline">\(\frac{\partial \theta}{\partial m11}\)</span> encapsulates the sensitivity of the loss function to changes in <code>m11</code>. A negative derivative suggests increasing <code>m11</code> reduces the loss, while a positive one implies the opposite. The update rule in training uses this derivative, along with the learning rate <code>η</code> to adjust <code>m11</code> effectively. This adjustment is in the opposite direction of the gradient to minimise the loss. Imagine the loss function as a hill; moving against the gradient (uphill direction) guides us downhill towards the loss minimum. Through iterative updates, the network ‘learns’ by tuning <code>m11</code> to enhance prediction accuracy.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>