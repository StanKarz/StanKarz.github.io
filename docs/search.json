[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my corner of the web.\nMy academic and professional journey has been anything but linear, taking me from the intricate world of Chemistry to the dynamic field of AI, drawn by its potential to redefine problem-solving and innovation. This interdisciplinary background has endowed me with a unique perspective and a versatile skill set, bridging the gap between the theoretical and the practical.\n\nRecent Activities\nThis past summer, my passion for AI led me to Seoul National University in South Korea, where I engaged in hands-on learning about AI in robotics during a summer exchange program. I had the opportunity to develop my technical skills in programming and computer vision but also embraced the cultural diversity and collaborative spirit of a global educational environment. This experience expanded my perspective on AI technologies’ universal impact and applications across cultures and industries.\n\n\nMost Recent Role\nBack in London, I embraced the opportunity to work as a Research Intern at University College London, in a dynamic research lab. I was tasked with a project that seemed like a prelude to a PhD journey: implementing model-free reinforcement learning techniques focusing on Q-learning algorithms and investigating the effectiveness of various methods for adversarial attacks. This endeavour allowed me to dive deep into the nuances of AI research, from conception to execution and presentation. It was a hands-on exploration of reinforcement learning’s potential and its challenges, further enhancing my skills in research methodologies and collaborative innovation.\n\n\nBeyond the Classroom\nPrior to my postgraduate studies, I worked as a Customer Success Specialist at Paperform, where I honed my skills in client engagement, technical support, and cross-functional collaboration. This role taught me the importance of customer-centric product development and the value of clear, accessible user education.\n\n\nSkills and Tools\nMy technical toolkit is extensive, with advanced proficiency in Python, SQL, and a suite of data science libraries (PyTorch, fastai, Pandas, Scikit-learn, NumPy, Matplotlib) that complement my research and development projects. I’m also well-versed in Git and Visual Studio Code, tools that underpin my collaborative projects and version control needs."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\nHere are some of the projects I’ve worked on:\n\nProject 1\nProject 2 … and so on."
  },
  {
    "objectID": "posts/git-explained/index.html",
    "href": "posts/git-explained/index.html",
    "title": "Git Explained",
    "section": "",
    "text": "Background\n\nIn today’s fast-paced software development world, Git is not just a tool; it’s a fundamental skill. Not only does it help teams track and coordinate complex projects but its distributed nature distinguishes it from other SCM (source code management) tools. It’s no surprise that Git powers the majority of modern software projects, playing a vital role in development workflows. This guide delves into the core features of Git along with concepts including staging, branching and merging.\n\n\nWhat is Git\n\nAt its simplest, Git is software that runs on your computer, recording and tracking every modification you make to your files. This is especially crucial for files containing code, as it enables developers to keep a detailed record of every change. But Git is much more than a simple tracker; as described by Wikipedia:\n\n“Git is a distributed version control system that tracks changes in any set of computer files, usually used for coordinating work among programmers who are collaboratively developing source code during software development.”\n\nThis definition introduces the term “distributed version control system”. To break this down, A VCS (version control system) is a name given to a system responsible for recording changes made to files over time. Unlike traditional VCS, which rely on a central server to store all versions of a project, Git operates on a distributed model. Every developer’s computer holds a complete copy of the project history, enabling work to continue seamlessly even without a central server connection.\n\nUnderstanding Version Control\n\nA VCS archives every modification made to a file or set of files over time. This allows anyone to revert to a previous version, compare changes, or analyse the evolution of a project. Git’s distributed nature not only enhances project security, since every participant has a full backup but also significantly improves collaboration and flexibility. Developers can work independently on different features or fixes and later “merge” their changes without the risk of losing work or interfering with the main codebase.\n\n\nThe Power of Git\n\nWith Git, every clone of a repository is not just a snapshot of your project at a given time; it’s a comprehensive backup of your project’s entire history. This aspect is fundamental for resilient project management and enables a wide array of workflows that can be tailored to the needs of any development team.\n\n\n\nGit Repositories\n\nAt the heart of Git’s powerful version control capabilities is the concept of repositories, or “repos” for short. A Git repository is essentially a database of your project’s history, tracking changes to your files over time. This history is stored in a special directory called the .git folder, which is created when you initialise Git in your project.\n\n\n\n\n\n\nNote\n\n\n\nThe .git folder contains more than just files; it houses the entire history of changes, branch information, and more.\n\n\n\nLocal vs. Remote\n\nA local repository resides on your personal computer, allowing you direct access to your files and their revision history. In contrast, a remote repository is stored on a server accessible over the internet or a network, facilitating collaboration with others. Platforms like GitHub, GitLab, and Bitbucket are popular choices for hosting remote repositories.\n\n\nCollaborating with Remote Repos\n\nRemote repositories enable teams to work together on a project. By cloning a remote repo to your local machine, you gain access to the most recent version of the project. Changes made locally can be shared with the team by pushing them to the remote repository, ensuring everyone stays in sync.\n\n\nCommitting and Pushing Changes\n\nA commit represents a snapshot of your project at a moment in time, allowing you to track the evolution of your code and revert to previous states if necessary. With descriptive commit messages, you can create a readable history of the project development. Pushing commits to a remote repository shares your changes with the team, facilitating collaboration.\n\n\n\n\n\n\nNote\n\n\n\nGit requires your local repo to be up to date with the remote repo before allowing you to push any changes, ensuring a conflict-free workflow. If your colleagues have pushed updates to the repo while you were working on it locally, Git will ask you to first “pull” these changes, integrating them with your work, before you can “push” your commits.\n\n\n\n\n\nKey Features of Git\n\nGit’s unique combination features sets it apart from other VCS. These features are designed to enhance efficiency, reliability and flexibility in the modern development process, making Git an indispensable tool. From its speed and distributed architecture to its sophisticated branching model and beyond, Git’s capabilities address the complex needs of today’s software projects. In this section, we delve into the specific features that contribute to Git’s standing as the preferred VCS for so many professionals. Each of these features not only solves practical problems faced in software development but also opens up new possibilities for how teams can work together on code.\n\nFast\n\nOne of the features of Git that make it fast is that each user has a full local history of a repository and can create commits and inspect the file history without an internet connection. Additionally, most operations are performed locally, which makes Git a faster VCS alternative than centralised systems that require constant communication with a remote server.\n\n\nDistributed\n\nAs mentioned earlier, when working in a team each member gets a full copy of the repository when they clone it, along with a full history of the commits so far. This means that in the event of a crash there is no single point of failure with Git, as every user has a copy of the main server (unless there is only a single copy of the repository).\nIn terms of development, a distributed system makes it easier to scale an engineering team. For instance with a centralised VCS, if a developer breaks the branch which contains the production code, other developers won’t be able to commit their changes until this is resolved. However, with Git this isn’t an issue, as everyone can continue working in their local repositories.\n\n\nGit’s branching model\n\nOne of the unique aspects of Git that distinguish it from other SCM tools is its branching capabilities. Think of a branch as an isolated environment for experimenting with a new feature, fixing a bug or testing out an idea. If we are satisfied we can go ahead and merge it with the main branch. A local branch exists on the local user’s machine; the main branch typically contains the production code.\n\n\n\nExample of a Git repository, each circle represents a commit.\n\n\nUsers are encouraged to utilise multiple local branches that can be independent of each other for the reasons stated above (disposable experimentation, working on new features and trying out new ideas).\nAdditionally, it is not necessary to push all your local branches to the remote repository, you can decide which ones to share. Ultimately, this gives users the freedom to try out new ideas and alleviates the worry of having to plan when they are going to share them with the rest of the team.\n\n\n\n\n\n\nNote\n\n\n\nCreation, merging and deletion of branches is extremely fast (takes seconds) and is a straightforward process.\n\n\n\n\nStaging\n\nGit’s “staging area” is a pre-commit holding area which allows you to review and format changes before making a commit. This gives developers the flexibility to selectively stage certain files without making a commit for all the modified files. This functionality extends to giving user’s the ability to stage and commit particular parts of files e.g. certain functions. This is particularly useful for keeping your project’s history organised and making sure each commit reflects a single logical change.\nWhen working with Git, files can be in either one of three states:\n\nModified: Changes have been made to the file but have not been staged.\nStaged: A modified file in it’s current version has been prepared for the next commit snapshot.\nCommitted: The file has been stored in the local database.\n\n\n\nAny workflow\n\nA workflow is a recommendation for how to utilise Git for all members to accomplish work productively and consistently. Git’s distributed nature and intuitive branching system make it possible for an almost endless number of workflows to be implemented with relative ease. As Git is focused on flexibility there is no standardised process for how to interact with Git.\n\n\nData Integrity\n\nGit ensures the integrity of code history through cryptographic hashes (SHA-1). Each commit is checksummed, guaranteeing the history’s integrity and immutability. Checksumming in Git involves generating a unique SHA-1 hash for each set of changes or commit. This hash acts as a fingerprint for that particular commit, encapsulating not only the changes made but also the context of those changes, including who made them, when they were made, and in relation to what other commits. This SHA-1 hash is a 40-character string composed of hexadecimal characters (0-9 and a-f) and represents a virtually unique identifier for that commit. This security feature is crucial for maintaining the reliability of a codebase, and is not commonly supported with centralised VCS."
  },
  {
    "objectID": "posts/git-explained/index.html#git-is-fast",
    "href": "posts/git-explained/index.html#git-is-fast",
    "title": "Git Explained",
    "section": "Git is fast",
    "text": "Git is fast\nOne of the features of Git that make it fast is that each user has a full local history of a repository and can create commits and inspect the file history without an internet connection. Additionally, most operations are performed locally, which makes Git a faster VCS alternative than centralised systems that require constant communication with a remote server."
  },
  {
    "objectID": "posts/git-explained/index.html#git-is-distributed",
    "href": "posts/git-explained/index.html#git-is-distributed",
    "title": "Git Explained",
    "section": "Git is distributed",
    "text": "Git is distributed\nAs mentioned earlier, when working in a team each member gets a full copy of the repository when they clone it, along with a full history of the commits so far. This means that in the event of a crash there is no single point of failure with Git, as every user has a copy of the main server (unless there is only a single copy of the repository).\nIn terms of development, a distributed system makes it easier to scale an engineering team. For instance with a centralised VCS, if a developer breaks the branch which contains the production code, other developers won’t be able to commit their changes until this is resolved. However, with Git this isn’t an issue, as everyone can continue working in their local repositories."
  },
  {
    "objectID": "posts/git-explained/index.html#gits-branching-model",
    "href": "posts/git-explained/index.html#gits-branching-model",
    "title": "Git Explained",
    "section": "Git’s branching model",
    "text": "Git’s branching model\nOne of the unique aspects of Git that distinguish it from other SCM tools is its branching capabilities. Think of a branch as an isolated environment for experimenting with a new feature, fixing a bug or testing out an idea. If we are satisfied we can go ahead and merge it with the master branch. A local branch exists on the local user’s machine; the main branch typically contains the production code.\n\n\n\nExample of a Git repository, each circle represents a commit.\n\n\nUsers are encouraged to utilise multiple local branches that can be independent of each other for the reasons stated above (disposable experimentation, working on new features and trying out new ideas).\nAdditionally, it is not necessary to push all your local branches to the remote repository, you can decide which ones to share. Ultimately, this gives users the freedom to try out new ideas and alleviates the worry of having to plan when they are going to share them with the rest of the team. Another important thing to mention is that the creation, merging and deletion of branches is extremely fast (takes seconds) and easy."
  },
  {
    "objectID": "posts/git-explained/index.html#any-workflow",
    "href": "posts/git-explained/index.html#any-workflow",
    "title": "Git Explained",
    "section": "Any workflow",
    "text": "Any workflow\nA workflow is a recommendation for how to utilise Git for all members to accomplish work productively and consistently. Git’s distributed nature and intuitive branching system make it possible for an almost endless number of workflows to be implemented with relative ease. As Git is focused on flexibility there is no standardised process for how to interact with Git."
  },
  {
    "objectID": "posts/how-to-use-git/index.html",
    "href": "posts/how-to-use-git/index.html",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "",
    "text": "Git is a distributed VCS (version control system) that tracks changes made to files and directories, enabling users to record project changes and go back to a specific version of tracked files at any given time. Since Git is mainly used via the command line interface, knowledge of relevant commands is crucial for using Git effectively."
  },
  {
    "objectID": "posts/how-to-use-git/index.html#background",
    "href": "posts/how-to-use-git/index.html#background",
    "title": "How to use Git",
    "section": "Background",
    "text": "Background\nGit is a distributed VCS (version control system) that tracks changes made to files and directories, enabling users to record project changes and go back to a specific version of tracked files at any given time. Since Git is mainly used via the command line interface, knowledge of relevant commands is crucial for using Git effectively.\nThere are two scenarios that we will walk through:\n\nUsing Git with a local repository to track changes made to an existing project on your local machine.\nDeveloping an existing remote project which exists outside of your local machine."
  },
  {
    "objectID": "posts/how-to-use-git/index.html#configuration",
    "href": "posts/how-to-use-git/index.html#configuration",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Configuration",
    "text": "Configuration\nLocal configuration variables are important, as they provide a way to identify who is making what changes to the project code. This is especially useful if you’re working with other developers. To specify your Git configuration settings, use the following commands:\ngit config --global user.name \"John Doe\"\ngit config --global user.email \"johndoe@example.com\""
  },
  {
    "objectID": "posts/how-to-use-git/index.html#initialising-a-new-git-repository",
    "href": "posts/how-to-use-git/index.html#initialising-a-new-git-repository",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Initialising a new Git Repository",
    "text": "Initialising a new Git Repository\nRunning the command:\ngit init\nWill create a hidden .git directory for your project, this is where Git stores all internal tracking for the current repository.\n\n\n\n\n\n\nNote\n\n\n\nBefore initialising a new Git repository ensure that you have navigated to the main folder that contains your project files.\n\n\nAfter initialising a new Git repository, you can use the command:\nls -a\nTo show all the hidden files and directories including the newly created .git directory, you should see a .git file."
  },
  {
    "objectID": "posts/how-to-use-git/index.html#working-directory",
    "href": "posts/how-to-use-git/index.html#working-directory",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Working Directory",
    "text": "Working Directory\nThe working directory consists of files you are currently working on, this is where untracked and modified files reside."
  },
  {
    "objectID": "posts/how-to-use-git/index.html#staging-area",
    "href": "posts/how-to-use-git/index.html#staging-area",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Staging Area",
    "text": "Staging Area\nYou can think of the staging area as a rough draft space where you organise and select all the files you want to be part of the next commit. For example, if you’ve made several changes across multiple files and you want to make multiple commits then you can stage the individual files and commit them in small chunks. This would make your commits more detailed and allow you to be more specific about what changes were made to the project.\n\nRepository\nOnce you are satisfied with the changes you’ve made you can go ahead and commit the staged files to your repository."
  },
  {
    "objectID": "posts/how-to-use-git/index.html#staging-files",
    "href": "posts/how-to-use-git/index.html#staging-files",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Staging files",
    "text": "Staging files\nWe can view the files in our working directory that have been modified or are being tracked by using:\ngit status\nThis command returns the current branch you are on (master branch). Untracked files are in red, tracked files appear in green.\n\nExample screenshot\nTo add a file to the staging area use the command:\ngit add fileName\nTo add multiple files to the staging area you can specify the names of all the files with a space between each file name:\ngit add fileName1 fileName2\nOr you could stage all the files and directories in your project folder by using:\ngit add .\nOnce you have added the relevant files to the staging area, you can use git status again to ensure the correct files are in the staging area.\n\n\nScreenshot here\nNow we can see which file or files have been added to the staging area and the changes are ready to be committed."
  },
  {
    "objectID": "posts/how-to-use-git/index.html#unstaging-files",
    "href": "posts/how-to-use-git/index.html#unstaging-files",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Unstaging Files",
    "text": "Unstaging Files\nTo remove a file from the staging area, use the following command.\ngit reset fileName\nAlternatively, you can use git reset to remove all files from the staging area."
  },
  {
    "objectID": "posts/how-to-use-git/index.html#making-commits",
    "href": "posts/how-to-use-git/index.html#making-commits",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Making Commits",
    "text": "Making Commits\nIn Git, commits are considered checkpoints or snapshots of your project at the current state, essentially you are saving the current version of your code with every commit that you make. You can create as many commits as you need and can go back and forth between different versions of the project. Commits are typically created at different logical points, such as after adding a new feature or a bug-fix implementation.\n\n\n\n\n\n\nNote\n\n\n\nBefore you can commit a file you need to place it in the staging area first.\n\n\nTo commit a file use the command git commit -m \"commit messge\""
  },
  {
    "objectID": "posts/how-to-use-git/index.html#commit-message",
    "href": "posts/how-to-use-git/index.html#commit-message",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Commit Message",
    "text": "Commit Message\n(Message that this is optional) The commit message should be a descriptive summary of the changes that you are committing to the repository. After you’ve made a commit you will see a summary displayed, similar to the one below."
  },
  {
    "objectID": "posts/how-to-use-git/index.html#commit-history",
    "href": "posts/how-to-use-git/index.html#commit-history",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Commit History",
    "text": "Commit History\nTo view all the commits made for a project you can use: git log\nThis will return details for each commit including the unique generated hash, author, date & time the commit was made, along with the commit message.\n[Example here]"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#reverting-to-a-previous-commit",
    "href": "posts/how-to-use-git/index.html#reverting-to-a-previous-commit",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Reverting to a previous commit",
    "text": "Reverting to a previous commit\nThe commit hash can be used to go back to a previous version of the project by using the command: git checkout &lt;commit_hash&gt;\nWhere commit_hash is the hash of a previous commit that you want to revert to. As mentioned above the commit hash of all the commits can be retrieved by using git log.\nYou can return to the latest version of the project files by using the command: git checkout main"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#creating-a-new-branch",
    "href": "posts/how-to-use-git/index.html#creating-a-new-branch",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Creating a new branch",
    "text": "Creating a new branch\nTo create a new branch, use the command: git branch &lt;new_branch&gt;\nIt’s good practice to create a development branch for adding new features and experimenting with code, once everything is working as expected you can merge it with the master branch."
  },
  {
    "objectID": "posts/how-to-use-git/index.html#switching-between-branches",
    "href": "posts/how-to-use-git/index.html#switching-between-branches",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Switching between branches",
    "text": "Switching between branches\nCreating a new branch does not automatically switch to that branch, to do this you will need to use the command: git checkout &lt;branch_name\nThe commits made on one branch will be independent of changes made on all other branches unless you decide to merge these changes later on. You can create a new branch and switch to it simultaneously by using git checkout with the -b flag: git checkout -b &lt;new_beanch&gt;\nTo return to the main branch, run: git checkout main"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#deleting-a-branch",
    "href": "posts/how-to-use-git/index.html#deleting-a-branch",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Deleting a branch",
    "text": "Deleting a branch\nIf you’re satisfied with the changes made on an individual branch you can merge with another branch by using:\ngit branch -d &lt;branch_name&gt;"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#merging-a-branch",
    "href": "posts/how-to-use-git/index.html#merging-a-branch",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Merging a Branch",
    "text": "Merging a Branch\nIf you’re satisfied with the changes made on an individual branch you can merge with another branch by using:\ngit merge &lt;branch_name&gt; For example, you may have implemented a bug fix and want to merge this with the stable branch of your code (usually the master branch). branch_name is the branch you want to integrate into the current one you are on."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stan Karzhev",
    "section": "",
    "text": "Backpropagation: The Essence of Neural Network Training\n\n\n\n\n\n\nDeep Learning\n\n\n\nA walkthrough on how the backpropagation algorithm is involved in training artificial neural networks.\n\n\n\n\n\nJan 31, 2024\n\n\nStan Karzhev\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Python Features for Data Science\n\n\n\n\n\n\nCode\n\n\n\nFeatures of Python that are useful in data science workflows with code examples.\n\n\n\n\n\nDec 20, 2023\n\n\nStan Karzhev\n\n\n\n\n\n\n\n\n\n\n\n\nA Guide to Git Commands: From Basic to Advanced\n\n\n\n\n\n\nDevelopment\n\n\n\n….\n\n\n\n\n\nNov 23, 2023\n\n\nStan Karzhev\n\n\n\n\n\n\n\n\n\n\n\n\nGit Explained\n\n\n\n\n\n\nDevelopment\n\n\n\nA brief overview on what makes Git the most widely used version control system for modern development.\n\n\n\n\n\nNov 5, 2023\n\n\nStan Karzhev\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/python-for data-science/index.html",
    "href": "posts/python-for data-science/index.html",
    "title": "Useful Python Features for Data Science",
    "section": "",
    "text": "Background\n\nPython stands at the forefront of data science and machine learning, not just because of its simplicity but also due to its powerful suite of functions and methods. We will explore some of the most useful functions including map(), filter(), and zip(), along with the elegance of list and dictionary comprehensions, the power of iterators and generators and the versatility of decorators. We’ll delve into their syntax, provide simple examples, and demonstrate their practical application in data science and machine learning to harness Python’s capabilities for data-driven projects.\n\n\n\n\n\n\nNote\n\n\n\nYou can run the code below in a Jupyter Notebook via Binder here\n\n\n\n\nmap() & filter()\n\nThe map() function takes two inputs, the function to apply and the iterable (e.g. list, set, dictionary, tuple, string) to apply it to, the returned result will be an iterator, in this case, a map object containing the new output. The input function can be any callable function including built-in functions, lambda functions, user-defined functions, classes and methods. Essentially, map() iterates over the iterable and applies the input function to each element in the iterable, just like a for loop might do.\nsyntax: map(function, iterable)\n## Simple example\n\n# Define a function to calculate the square of a number\ndef square(x):\n    return x ** 2\n\n# Create a list of integers\nnumbers = [1, 2, 3, 4, 5]\n\n# Apply the square function to each element in the map object and convert the result to a list\nsquared_numbers = list(map(square, numbers))\nsquared_numbers  # Output: [1, 4, 9, 16, 25]\n\n\n\n\n\n\nNote\n\n\n\nObjects returned by map and filter are iterators which means their values are not stored but generated as needed.\n\n\nIn this example, we demonstrate the use of map() in a typical data processing scenario common in machine learning. The goal here is to normalise a list of height data. Normalisation, especially Z-score normalisation, is a crucial step in preparing data for many machine-learning algorithms. It involves scaling the data so that it has a mean of 0 and a standard deviation of 1. Therefore, all features contribute equally to the model’s performance.\n## Example in Data Preprocessing \n\nheights = [170, 155, 180, 190, 162]\n\n# Function to calculate the mean and standard deviation\ndef calculate_mean_std(data):\n    mean = sum(data) / len(data)\n    std = (sum([(x - mean) ** 2 for x in data]) / len(data)) ** 0.5\n    return mean, std\n\nmean_height, std_height = calculate_mean_std(heights)\n\n# Define a function to normalise the data\ndef normalize(x):\n    return (x - mean_height) / std_height\n\nnormalized_heights = list(map(normalize, heights))\nnormalized_heights # Output: [-0.112, -1.313, 0.688, 1.489, -0.752]\nfilter() is a function that selectively passes elements from an iterable through a test function. Like map(), it takes a function and an iterable as arguments. However, instead of transforming each element, filter() checks each element against the test function and returns only those for which the function evaluates to True. In simple terms, filter() sifts through an iterable, keeping only the elements that meet a specified condition.\nsyntax: filter(function, iterable)\nIn a machine learning context, we can use filter() for feature selection, data cleaning or removing outliers. For instance, we might want to filter out abnormal values which could be due to errors or outliers. These readings are crucial for training a ML model and removing them is a common preprocessing step.\n\n# Sample data: temperature readings in Fahrenheit\nsensor_readings = [102, 95, 101, 150, 99, 98, 200, 105]\n\n# Function to convert Fahrenheit readings to Celsius\ndef fahrenheit_to_celsius(f):\n    return (f - 32) * 5 / 9\n\n# Function to check if the temperature is within the normal range\ndef is_normal_temp(c):\n    return -5 &lt;= c &lt;= 40\n\n# First, convert all readings from Fahrenheit to Celsius using map\ncelsius_readings = list(map(fahrenheit_to_celsius, sensor_readings))\ncelsius_readings # Output: [38.8, 35.0, 38.3, 65.5, 37.2, 36.6, 93.3, 40.5]\n\n\n# Next, filter out abnormal readings using filter\nnormal_celsius__readings = list(filter(is_normal_temp, celsius_readings))\nnormal_celsius__readings # Output: [38.8, 35.0, 38.3, 37.2, 36.6]\nOverall, map() & filter() provide both a more Pythonic and efficient way of handling data transformations and filtering. They are considered a more elegant alternative to traditional loops, aligning with Python’s emphasis on readability and simplicity.\n\n\n\n\n\n\nNote\n\n\n\nNote: “Pythonic” refers to a coding style and practice that leverages Python’s unique features to write code that is clear, concise and readable. For comprehensive guidelines on adhering to these conventions refer to the PEP 8 Style Guide\n\n\n\n\nlambda functions\n\nlambda() functions, also known as anonymous functions are defined using the lambda keyword and are especially useful as one-off functions that don’t need to be named or reused. They are typically used where functions are required for a short period within a larger expression and are best used sparingly, in situations where they enhance readability and conciseness.\nsyntax: lambda parameters: expression\nPreviously, we used both map() and filter() in conjunction with user-defined functions, however, we can achieve the same result using lambda. This approach is more concise, eliminating the need for a separate function definition.\n\n# Method 1 using def\n\n# Define a function to calculate the square of a number\ndef square(x):\n    return x ** 2\n\n# Create a list of integers\nnumbers = [1, 2, 3, 4, 5]\n\n# Apply the square function to each element in the map object and convert the result to a list\nsquared_numbers_1 = list(map(square, numbers))\n\n# Method 2 using lambda\nsquared_numbers_2 = list(map(lambda x: x ** 2, numbers))\nIn this example, lambda performs the same check as even() within the filter call. This approach is more streamlined and avoids the overhead of defining and naming a separate function.\n# Method 1 using def\ndef even(x):\n    return x % 2 == 0\n\nnumbers = [1, 2, 3, 4, 5]\n\n# Apply the even function to each element in the filter object and convert the result to a list \nfiltered_numbers_1 = list(filter(even, numbers))\n\n# Method 2 using lambda\nfiltered_numbers_2 = list(filter(lambda x: x% 2 ==0, numbers))\nDespite the ability of lambda to make code more readable and expressive, particularly when utilised in common programming patterns involving map(), filter() and sorted(), lambda functions are not a one-size-fits-all solution and should not be used judiciously to maintain code clarity. Here’s an example of when the use of lambda would not be appropriate:\n# Using def \ndef process_data(data):\n        # Check if data is empty or None, raise an error if True\n    if not data:\n        raise ValueError(\"No data provided\")\n        # Apply a complex data transformation to each data item if a condition is met\n    transformed = [complex_transformation(d) for d in data if condition(d)]\n        \n        # Aggregate transformed data\n    result = aggregate(transformed)\n        \n        #Post process aggregated result\n    post_processed_result = post_process(result)\n\n    return post_processed_result\n# Using lambda\nprocess_data_lambda = lambda data: post_process(aggregate([complex_transformation(d) for d in data if condition(d)])) if data else ValueError(\"No data provided\")\n\n\nList & Dictionary Comprehensions\n\nList and dictionary comprehensions are concise ways to create lists and dictionaries in Python. They offer a more readable and efficient way to generate these collections compared to traditional loops and functions. Comprehensions follow the principle of writing simple expressions that transform or filter sequence elements.\nsyntax: new_list = [expression for item in iterable if condition == True]\nsyntax: new_dict = {key, value for key,value in iterable if condition == True}\n\n\n\n\n\n\nNote\n\n\n\nThe expression can be any function or operation applied to the item, also the condition checking aspect of list and dict comprehensions is optional.\n\n\nIn some simple cases, using a list comprehension with a conditional can be thought of as using map() and filter() in conjunction. As the example below shows, we are squaring all even elements in a specified range.\n# Simple example\nsquares = [x**2 for x in range(5) if x % 2 == 0]\nsquares # Output: [0, 4, 16]\nThe equivalent approach using map() and filter() looks like this, it’s evident that the list comprehension is more readable and succinct, making it a preferred choice in such scenarios.\nsquares = map(lambda x: x**2, filter(lambda x: x % 2 == 0, range(5)))\nsquares = list(squares)  # Output: [0, 4, 16]\nFor dictionary comprehension, the syntax is similar except we use curly braces {} instead of square brackets [] which are typically used for lists instead, and in this case, we omitted the optional conditional.\n# Simple example \n# Dictionary inversion, keys become values and values become keys\n\nmy_map = {\"a\": 1, \"b\" :2, \"c\" : 3}\n\ninverted_map = {value: key for key, value in my_map.items()}\ninverted_map # Output: {1: 'a', 2: 'b', 3: 'c'}\nIn many machine learning models, especially those based on algorithms that require numerical input, you need to convert categorical data (like colour names) into numerical form. One simple approach is to create a binary encoding for a categorical feature, which can be simply implemented with a list or dict comprehension.\n# ML related example\n\ncolours = ['red', 'green', 'blue', 'yellow', 'red']\n\nencoded_colours = [1 if colour == 'red' else 0 for colour in colours]\nencoded_colours # Output: [1, 0, 0, 0, 1]\nThis example demonstrates a simple one-hot encoding scenario. List comprehension makes it not only readable but also faster, which is crucial when encoding large datasets typical in machine learning tasks.\ncategories = ['apple', 'orange', 'apple', 'pear', 'orange', 'banana']\n\ncategory_encoding = {category: idx for idx, category in enumerate(set(categories))}\ncategory_encoding # Output: {'orange': 0, 'banana': 1, 'pear': 2, 'apple': 3}\nIn summary, list and dictionary comprehensions are not only about writing more concise code; they also offer performance benefits. These benefits stem from the way Python optimises comprehensions at the bytecode level. When a comprehension is compiled, Python generates specialised bytecode. This bytecode is inherently more efficient for executing loops and performing condition checks, compared to the more general-purpose bytecode generated for loops with embedded if-else blocks. The reason behind this efficiency is the predictable pattern of comprehensions—they consistently involve iteration and, often, condition application. This predictable structure allows Python to streamline the execution process at the bytecode level, resulting in faster performance.\n\n\nzip()\n\nThe zip() function accepts iterators as its arguments and returns a zip object (an iterator of tuples) where the first item from each passed iterator is paired together, and then the second item in each passed iterator are paired together and so on until we reach the length of the iterable with the least items, this also decides the length of the output iterator. Essentially, the function returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the input iterators.\nsyntax: zip(iterator_1, iterator_2, iterator_3)\n# Simple example pairing fruis with their colours\n\nfruits = ['apple', 'banana', 'grape', 'pear']\ncolours = ['red', 'yellow', 'purple']\n\npaired_fruits = zip(fruits,colours)\n\nlist(paired_fruits) \n# Output: [('apple', 'red'), ('banana', 'yellow'), ('grape', 'purple')]\n\n\n\n\n\n\nNote\n\n\n\nThere’s no restriction on the number of iterators you can provide to Python’s zip() function as input arguments.\n\n\nSuppose you are working on a machine learning problem where you need to combine features from different datasets or feature sets. For instance, you might have one set of features coming from survey data and another set from transactional data, and you need to pair these features for each individual in your dataset. This is where the zip() function comes into use.\nsurvey_features = [(25, 'M'), (30, 'F'), (22, 'F')]  # Example: (Age, Gender)\ntransactional_features = [(1200, 300), (2000, 500), (1500, 400)]  # Example: (Annual Spending, Number of Transactions)\n\n# Combine features using zip\ncombined_features = zip(survey_features, transactional_features)\n\nfor survey, transaction in combined_features:\n    print(f\"Survey Data: {survey}, Transactional Data: {transaction}\")\n    \n# Output:  Survey Data: (25, 'M'), Transactional Data: (1200, 300)\n#          Survey Data: (30, 'F'), Transactional Data: (2000, 500)  \n#          Survey Data: (22, 'F'), Transactional Data: (1500, 400)\nIn this example, zip() is used to combine data from two different feature sets, creating a paired iterable. For each individual, you now have a tuple containing both their survey data and transactional data. This can be particularly useful in feature engineering, where you might need to create a unified feature set from different sources.\n\n\nIterators & Generators\n\nAs previously discussed, functions like map and filter in Python return iterator objects. This ties back to our discussion on iterators, which are fundamental to Python’s handling of sequences and collections. An iterator in Python is an object that can be iterated over, meaning you can traverse through all the values it holds. This is facilitated by the __iter__() method, which returns the iterator object itself, and the __next__() method, which retrieves the next value from the iterator and raises a StopIteration exception when there are no more values.\n# Simple example\n\nclass CountDown():\n    def __init__(self, start):\n        self.current = start\n    \n    def __iter__(self):\n        return self\n    \n    def __next__(self):\n        if self.current &lt;= 0:\n            raise StopIteration\n        else:\n            self.current -= 1\n            return self.current\n\nfor number in CountDown(3):\n    print(number)\n# Output: 2 1 0\n\n\n\n\n\n\nNote\n\n\n\nIterators are particularly useful in handling large data streams that might not fit entirely in memory.\n\n\n# ML related example\n\nclass BatchIterator():\n    \"\"\"\n    An iterator that yields batches of data from a dataset.\n    This is useful in machine learning for processing large datasets in mini-batches.\n    \"\"\"\n    def __init__(self, dataset, batch_size):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.index = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.index &gt;= len(self.dataset):\n            raise StopIteration\n        batch = self.dataset[self.index:self.index + self.batch_size]\n        self.index += self.batch_size\n        return batch\nGenerators are a more concise way to create iterators using functions. A generator is a function that behaves like an iterator, generating a sequence of values using the yield keyword. Unlike regular functions that return a single value and then terminate, generators can maintain state in local variables across multiple calls, making them ideal for efficient data processing, here’s some examples:\n# Simple example\n\ndef countdown_generator(start):\n    current = start\n    while current &gt; 0:\n        yield current\n        current -= 1\n\nfor number in countdown_generator(3):\n    print(number)\n# Output: 3 2 1\n# ML related example\n\ndef data_augmentation_generator(dataset, augment_func):\n    \"\"\"\n    A generator for performing data augmentation on the fly.\n    This is useful in machine learning for augmenting data during training without\n    needing to store the augmented data in memory.\n    \"\"\"\n    for data in dataset:\n        yield augment_func(data)\n\nOverall\n\n\nIterators are objects that support the iterator protocol (__iter__ and __next__ methods), while generators are functions that yield values using yield.\nCreation: Iterators are created using classes, requiring explicit definitions of __iter__ and __next__, whereas generators are created using functions, which is generally more concise.\nUse Case: Iterators are ideal for more complex or stateful iteration, while generators are suitable for simpler cases where a function can manage the state.\nCommonality: Both iterators and generators provide a way to handle large datasets efficiently, enabling the processing of data that doesn’t fit in memory, and both follow lazy evaluation, computing one element at a time on demand.\n\n\n\n\nDecorators\n\nA decorator takes an existing function as an argument and extends its behaviour without explicitly modifying it. Decorators are a powerful feature that allows for the modification or enhancement of functions or methods in a clean, readable and maintainable way.\n\nUses\n\n\nCode Reusability and Separation of Concerns: Decorators can add functionality to existing functions or methods, allowing for code reuse and separation of concerns.\nLogging and Debugging: They are widely used for logging function calls, which is helpful for debugging.\nPerformance Testing: Decorators can be used for timing functions, which is crucial in performance testing and optimisation.\nData Processing and Pipeline Setup: In data science, decorators can streamline the setup of data processing pipelines, making the code cleaner and more modular.\n\n# Simple example\n\ndef my_decorator(func):\n    def wrapper():\n        print('Something is happening before the function is called.')\n        func()\n        print('Something is happening after the function is called.')\n    return wrapper\n\n@my_decorator\ndef say_hello():\n    print('Hello!')\n\nsay_hello()\n\n# Output:\n# something is happening before the function is called.\n# hello!\n# something is happening after the function is called.\n# Simple example\n\ndef my_decorator(func):\n    def wrapper():\n        print('Something is happening before the function is called.')\n        func()\n        print('Something is happening after the function is called.')\n    return wrapper\n\n@my_decorator\ndef say_hello():\n    print('Hello!')\n\nsay_hello()\n\n# Output:\n# something is happening before the function is called.\n# hello!\n# something is happening after the function is called.\nIn machine learning and data science, decorators have practical applications in performance monitoring, logging, and setting up data processing pipelines. By understanding and utilising decorators, you can write more efficient and Pythonic code."
  },
  {
    "objectID": "posts/backpropagatio/index.html",
    "href": "posts/backpropagatio/index.html",
    "title": "Backpropagation: The Essence of Neural Network Training",
    "section": "",
    "text": "What is Backpropagation\nImagine teaching a robot to distinguish between cats and other animals, each time it makes a mistake you adjust its “thought process” slightly, to make it better at this task. In artificial intelligence, this fine-tuning is achieved through an algorithm known as backpropagation. It’s a method that iteratively adjusts a neural network’s parameters, steering it towards higher accuracy.\nBut how does backpropagation know which way to adjust? It uses calculus, specifically by finding the gradient with respect to a loss function. This gradient acts like a compass, pointing towards the direction where the network’s output or predictions become more accurate. This process is not just limited to neural networks; it’s a general technique applicable to various mathematical expressions, making it a universal tool in machine learning.\nSince its inception, backpropagation has been pivotal in the resurgence and success of neural networks, marking a key milestone in the AI revolution. In the upcoming sections, we’ll explore the building blocks of neural networks, the intuition and math behind backpropagation and how it’s applied to train neural networks.\n\n\nArtificial Neural Networks\nArtificial neural networks (ANNs) are designed to emulate the way biological neurons in the human brain process information. These networks comprise interconnected nodes or neurons arranged in layers, communicating through connections akin to biological synapses. Each neuron in an ANN receives and processes data, contributing to the overall task of the network, such as classification or pattern recognition. By mimicking these biological processes, ANNs harness the complex, adaptive nature of the human brain to solve diverse computational problems.\n\nUnderstanding the Neural Network Architecture\n[Diagram here]\nBefore going into the details of backpropagation, it’s essential to grasp how neural nets are structured and the concept of forward propagation. Consider a simple neural network that consists of a few layers:\n\nInput Layer: This is where the network takes in data. The nodes x1 and x2 represent the features of this input data. A bias unit is often included in this layer to account for offset in the decision boundary.\nHidden Layer: In this layer, the input data is processed by the neurons a1, a2 and a3. Each connection from the input to the hidden layer is weighted \\(W_{ij}\\) , the strength of these weights signifies the importance of the input values to the neurons.\nActivation Function: Each neuron in the hidden layer processes input weighted data by summing them and applying an activation function. This aspect is crucial as these functions introduce non-linearity in the network and enable the network to capture complex patterns. Without them, the neural network would become a linear regressor, regardless of how many layers we add.\nOutput Layer: The neuron y1 receives processed information from the hidden layer neurons and through weighted connections \\(M_{1j}\\), generates the final output after applying an activation function.\n\n\n\nFrom Inputs to Hidden Neurons\nNow that we are familiar with the structure of a neural net we can perform forward propagation to obtain the output. Each neuron in the hidden layer calculates a weighted sum of its inputs and a bias term. This sum, known as the neuron’s activation potential, is then transformed by an activation function, which determines the neuron’s output. For neuron a1, the activation potential is calculated as follows:\n\\(activation_1 = x1 \\cdot + x2 \\cdot w12 + w10\\)\nThis result is then passed through an activation function to determine the neuron’s output.\n\\(a_1 = f(activation_1)\\)\n\n\n\n\n\n\nNote\n\n\n\nWeights can be positive (excitatory, amplifying the signal) or negative (inhibitory, reducing the signal), affecting the neuron’s activation.\n\n\n\n\nDiving Deeper into Activation Functions\nWhile we’ve previously mentioned how activation functions introduce non-linearity, let’s explore specific use cases and characteristics of common functions such as sigmoid, Tanh and ReLU.\n\nSigmoid: The sigmoid function’s smooth S-shaped curve allows it to approximate binary functions closely. It is continuous and differentiable, which makes it suitable for gradient-based optimisation methods. However, it can result in vanishing gradients, where gradients shrink and their derivates become tiny, resulting in negligible parameter updates and effectively halting the network from further training. This is especially problematic in deep networks with many layers.\nTanh: Often used in hidden layers where data is zero-centered; it helps in transitioning data effectively through layers without shifting the mean, which can accelerate learning. However, Tanh also suffers from vanishing gradients, although to a lesser extent than the sigmoid due to its symmetric output range. This range can help with the convergence in training because it tends to centre the output of the neurons. Neurons in later layers of deep networks start with activations that are not too small or too large, which can make learning more stable in the early stages.\nReLU: ReLU, short for Rectified Linear Unit, is widely used in deep neural networks due to its ability to facilitate faster and more effective learning, particularly in layers deep within the network. It directly counters the vanishing gradient problem by allowing positive gradients to flow through without change, which accelerates convergence during training. Despite its advantages, ReLU can be susceptible to “neuron death” during training, where neurons may stop participating in the data transformation process if their gradients reach zero, known as the “dying ReLU” issue.\n\n[diagram here]\n\n\nDemonstrating a Forward Pass\nGoing back to our original diagram and using the concepts covered so far, namely layers, weights, biases and activation functions we will now complete a forward pass to obtain the output of our neural network using the inputs, network parameters and sigmoid activation function.\n\nInputs: \\(x1 = 1, x2 = -1\\)\n\n\n\n\nParameter\nValue\n\n\n\n\n\\(w11\\)\n1\n\n\n\\(w21\\)\n0.5\n\n\n\\(w12\\)\n-0.5\n\n\n\\(w22\\)\n1\n\n\n\\(w31\\)\n0\n\n\n\\(w32\\)\n0.5\n\n\n\\(m11\\)\n0.25\n\n\n\\(m12\\)\n-1\n\n\n\\(m13\\)\n1\n\n\n\\(w10\\)\n0.10\n\n\n\\(w20\\)\n0.20\n\n\n\\(w30\\)\n-0.1\n\n\n\\(m10\\)\n1\n\n\n\n\n\\(f(z)  \\frac 1 {1+e^{-z}}\\)\n\n\\(a_1 = f(x1 \\cdot w11 + x2 \\cdot w12 +  w10) = 0.83\\)\n\\(a_2 = f(x1 \\cdot w21 + x2 \\cdot w22 +  w20) =\n0.430\\)\n\\(a_3 = f(x1 \\cdot w31 + x2 \\cdot w32 +  w30) = 0.354\\)\n\\(y_1 = f(a1 \\cdot m11 + a2 \\cdot m12 + a3 \\cdot m13 + m10) = 0.532\\)\n\n\nLoss Function\nNow that we know the output from our network we need to evaluate the accuracy of its prediction, a way to quantify how close we are to the true value. This is where the loss function comes into play. The loss function measures the disparity between the predicted output of the neural network and the actual output. By calculating this disparity, we can determine how well the network is performing and use it to guide the backpropagation process for further improvement. For this example, we will use mean squared error as our loss.\n\\(\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\\)\nWhere:\n\n\\(n\\) is the number of samples\n\\(Y_i\\) is the observed value\n\\(\\hat Y_i\\)\n\nFor instance, if the actual value of y is 1 and we know that our neural network predicted 0.532 then we can find the error associated with this output:\n\\(MSE = (1 - 0.532)^2 = 0.219\\)\n\n\n\n\n\n\nNote\n\n\n\nIn this case, we consider the number of samples to be 1, but this usually won’t be the case as the error will be averaged over many samples in the training set.\n\n\n\n\nOther Loss Functions\nSelecting an appropriate loss function is another important consideration when training a neural network, this usually depends on the type of task and has significant implications for the convergence and performance of the neural network. Here are a few other common loss functions:\nMean Absolute Error (MAE):\n\nUse Cases: Regression tasks\nAdvantages: Robust to outliers as errors are not squared in the calculation.\nDisadvantages: Because the errors are not squared this ****can understate the impact of large errors, which may not always be desirable.\nMathematical Expression:\n\n\\(MAE = \\frac 1N \\sum_{i=1}^{N} |y_i -\\hat y|\\)\nBinary Cross Entropy (Log Loss)\n\nUse Cases: Binary classification tasks.\nAdvantages: Well suited for measuring the performance of a classification model whose output is a probability value between 0 and 1.\nDisadvantages: Can be sensitive to imbalanced datasets where the number of instances of a class significantly outnumber the other.\nMathemtical Expression: \\(L_{\\text{BCE}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left [(Y_i \\cdot \\log\\hat{Y}_i + (1 - Y_i) \\cdot \\log(1 - \\hat{Y}_i)) \\right]\\)\n\nCategorical Cross-Entropy:\n\nUse Cases: Multi-class classification tasks.\nAdvantages: Generalises binary cross-entropy to multiple classes and is effective when each sample belongs to exactly one class.\nDisadvantages: Like binary cross-entropy, it can be sensitive to imbalanced datasets.\nMathematical Expression:\n\n\\(L_{CCE} = \\frac 1N \\sum_{i=1}^{N} \\sum_{c=1}^{M}y_{ic}\\cdot log(\\hat y_{ic})\\)\nWhere: \\(M\\) is the number of classes\nHinge Loss\n\nUse Cases: Binary classification tasks, especially for support vector machines (SVMs).\nAdvantages: It encourages the model to not only make the correct prediction but also to make it with high confidence.\nDisadvantages: Not suitable for probability estimates, as it does not model probability distributions.\nMathematical Expression:\n\n\\[\nL_{Hinge} = \\frac 1N \\sum_{i=1}^{N} max(0, 1 - y_i \\cdot \\hat y_i)\n\\]\n\n\n\nLearning by Reversing\nWith our neural network’s performance quantified by the loss function, we now face the challenge of improving it. This is where backpropagation comes into play. Backpropagation is a methodical way of adjusting the weights in our network to minimise the loss. By calculating how the loss changes with respect to each weight, we can adjust the weights in the direction that reduces the loss, iteratively enhancing the network’s predictions. By repeatedly applying forward and backpropagation across multiple iterations over the entire dataset, the network ‘learns’ a more precise model of the input-output relationship. Ultimately, this allows the network to incrementally capture the underlying patterns in the data, leading to more accurate predictions\n\nThe Chain Rule\nThe chain rule from calculus is expressed as:\n\\[\n\\frac {dy}{dx} = \\frac {dy}{du} \\cdot \\frac {du}{dx}\n\\]\nThe chain rule helps us understand how two rates of change are related. Imagine \\(y\\) depends on \\(u\\) and \\(u\\) depends on \\(x\\). If you know how quickly \\(y\\) changes with \\(u\\) and how quickly \\(u\\) changes with \\(x\\), then the chain rule allows us to multiply these two rates to find out the rate of change of \\(y\\) with respect to \\(x\\). It’s like figuring out the total effect on \\(y\\) when \\(x\\) changes, by linking their relationship through \\(u\\).\n\n\nComputing Gradients\nWith the chain rule in mind, we can calculate the gradient of the loss function with respect to each weight in the neural network. Essentially, backpropagation reverses the flow of computation in the neural network. Starting from the output it propagates the error back through the network, layer by layer. At each neuron, the chain rule is used to find out to what extent weights and biases need to be adjusted to reduce the error, linking the rate of change of the error with respect to the output (which we want to minimise) to the rate of change of the output with respect to the weights and biases (which we can control). This linkage allows us to “backpropagate” the error and update our weights to improve our model.\nFor instance, applying the chain rule for the weight \\(m11\\) gives us the expression:\n\\[ \\frac{\\partial \\theta}{\\partial m11} = \\frac{\\partial \\theta}{\\partial y1} \\cdot \\frac{\\partial y1}{\\partial net} \\cdot \\frac{\\partial net}{\\partial m11}\n\\]\nWhere:\n\n\\(y1 = f(net)\\)\n\\(net = a1 \\cdot m11 + a2 \\cdot m12 + a3 \\cdot m13 + m10\\)\n\\(\\theta\\) is the loss function\n\nThe weight m11 directly influences the output of the neural network by scaling the activation a1 from the hidden layer before it contributes to the output neuron y1. In essence, m11 modulates how much a1 impacts the final prediction. By adjusting m11, we can control the strength of this influence. During the training process, we seek to find the optimal value of m11 that minimises the loss function.\nGoing further back in the network, we can derive the expression for \\(w22\\) too:\n\\(\\frac{\\partial \\theta}{\\partial w22} = \\frac{\\partial \\theta}{\\partial y1} \\cdot \\frac{\\partial y1}{\\partial f(a2)} \\cdot \\frac{\\partial f(a2)}{\\partial a2} \\cdot \\frac{\\partial a2}{\\partial w22}\\)\nThe weight w22 plays a pivotal role in determining the output of a neural network by influencing the activation of neuron a2 in the hidden layer. A change in w22 alters the weighted input to a2, which, due to the sigmoid function, significantly affects a2's activation level. This change is then carried forward, impacting the input and, subsequently, the activation of the output neuron y1. Since the output y1 is a result of applying the sigmoid function to its total input, any modification in w22 leads to a non-linear alteration in y1. This alteration cascades to affect the network’s loss θ, akin to a domino effect, where adjusting the initial element w22, steers the network towards a lower loss and improved performance.\n\n\nUpdating Parameters\nThe computed gradients are used to make small adjustments to the weights and biases, a process guided by the learning rate, a hyperparameter that controls the size of these adjustments. This step is crucial; too large a learning rate can lead to overshooting the minimum of the loss function, while too small a learning rate can cause the training process to stall.\nOur update equation for w11 becomes:\n\\(m11 = m11 - \\alpha \\left[\\frac{\\partial \\theta}{\\partial y1} \\cdot \\frac{\\partial y1}{\\partial net} \\cdot \\frac{\\partial net}{\\partial m11}\n\\right]\\)\nWhere:\n\n\\(\\alpha\\) is the learning rate\n\nThe partial derivative \\(\\frac{\\partial \\theta}{\\partial m11}\\) encapsulates the sensitivity of the loss function to changes in m11. A negative derivative suggests increasing m11 reduces the loss, while a positive one implies the opposite. The update rule in training uses this derivative, along with the learning rate η to adjust m11 effectively. This adjustment is in the opposite direction of the gradient to minimise the loss. Imagine the loss function as a hill; moving against the gradient (uphill direction) guides us downhill towards the loss minimum. Through iterative updates, the network ‘learns’ by tuning m11 to enhance prediction accuracy."
  },
  {
    "objectID": "posts/backpropagation/index.html",
    "href": "posts/backpropagation/index.html",
    "title": "Backpropagation: The Essence of Neural Network Training",
    "section": "",
    "text": "Imagine teaching a robot to distinguish between cats and other animals, each time it makes a mistake you adjust its thought process slightly, to make it better at this task. In artificial intelligence, this fine-tuning is achieved through an algorithm known as backpropagation. It’s a method that iteratively adjusts a neural network’s parameters, steering it towards higher accuracy.\nBut how does backpropagation know which way to adjust? It uses calculus, specifically by finding the gradient with respect to a loss function. This gradient acts like a compass, pointing towards the direction where the network’s output or predictions become more accurate. This process is not just limited to neural networks; it’s a general technique applicable to various mathematical expressions, making it a universal tool in machine learning.\nSince its inception, backpropagation has been pivotal in the resurgence and success of neural networks, marking a key milestone in the AI revolution. The upcoming sections, will explore the building blocks of neural networks, the intuition and math behind backpropagation and it’s application in training neural networks."
  },
  {
    "objectID": "posts/backpropagation/index.html#other-loss-functions",
    "href": "posts/backpropagation/index.html#other-loss-functions",
    "title": "Backpropagation: The Essence of Neural Network Training",
    "section": "Other Loss Functions",
    "text": "Other Loss Functions\nSelecting an appropriate loss function is another important consideration when training a neural network, this usually depends on the type of task and has significant implications for the convergence and performance of the neural network. Here are a few other common loss functions:\n\nMean Absolute Error (MAE)\n\n\nUse Cases: Regression tasks.\nAdvantages: Robust to outliers as errors are not squared in the calculation.\nDisadvantages: Because the errors are not squared this can understate the impact of large errors, which may not always be desirable.\nMathematical Expression: \\(MAE = \\frac 1N \\sum_{i=1}^{N} |y_i -\\hat y|\\)\n\n\n\nBinary Cross Entropy (Log Loss)\n\n\nUse Cases: Binary classification tasks.\nAdvantages: Well suited for measuring the performance of a classification model whose output is a probability value between 0 and 1.\nDisadvantages: Can be sensitive to imbalanced datasets where the number of instances of a class significantly outnumber the other.\nMathematical Expression: \\(L_{\\text{BCE}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left [(Y_i \\cdot \\log\\hat{Y}_i + (1 - Y_i) \\cdot \\log(1 - \\hat{Y}_i)) \\right]\\)\n\n\n\nCategorical Cross-Entropy\n\n\nUse Cases: Multi-class classification tasks.\nAdvantages: Generalises binary cross-entropy to multiple classes and is effective when each sample belongs to exactly one class.\nDisadvantages: Like binary cross-entropy, it can be sensitive to imbalanced datasets.\nMathematical Expression: \\(L_{CCE} = \\frac 1N \\sum_{i=1}^{N} \\sum_{c=1}^{M}y_{ic}\\cdot log(\\hat y_{ic})\\)\n\nWhere: \\(M\\) is the number of classes\n\n\nHinge Loss\n\n\nUse Cases: Binary classification tasks, especially for SVMs (support vector machines).\nAdvantages: It encourages the model to not only make the correct prediction but also to make it with high confidence.\nDisadvantages: Not suitable for probability estimates, as it does not model probability distributions.\nMathematical Expression: \\(L_{Hinge} = \\frac 1N \\sum_{i=1}^{N} max(0, 1 - y_i \\cdot \\hat y_i)\\)"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#git-basics",
    "href": "posts/how-to-use-git/index.html#git-basics",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Git Basics",
    "text": "Git Basics\n…."
  },
  {
    "objectID": "posts/how-to-use-git/index.html#working-with-branches",
    "href": "posts/how-to-use-git/index.html#working-with-branches",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Working with Branches",
    "text": "Working with Branches"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#working-with-remote-repos",
    "href": "posts/how-to-use-git/index.html#working-with-remote-repos",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Working with Remote Repos",
    "text": "Working with Remote Repos"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#branch-strategies",
    "href": "posts/how-to-use-git/index.html#branch-strategies",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Branch Strategies",
    "text": "Branch Strategies"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#stashing-changes",
    "href": "posts/how-to-use-git/index.html#stashing-changes",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Stashing Changes",
    "text": "Stashing Changes"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#merge-conflicts",
    "href": "posts/how-to-use-git/index.html#merge-conflicts",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Merge Conflicts",
    "text": "Merge Conflicts"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#tags-for-releases",
    "href": "posts/how-to-use-git/index.html#tags-for-releases",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Tags for Releases",
    "text": "Tags for Releases"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#cleaning-up",
    "href": "posts/how-to-use-git/index.html#cleaning-up",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Cleaning Up",
    "text": "Cleaning Up"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#rebasing",
    "href": "posts/how-to-use-git/index.html#rebasing",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Rebasing",
    "text": "Rebasing"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#cherry-picking",
    "href": "posts/how-to-use-git/index.html#cherry-picking",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Cherry-Picking",
    "text": "Cherry-Picking"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#working-with-.gitignore-and-.gitattributes",
    "href": "posts/how-to-use-git/index.html#working-with-.gitignore-and-.gitattributes",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Working with .gitignore and .gitattributes",
    "text": "Working with .gitignore and .gitattributes"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#git-hooks",
    "href": "posts/how-to-use-git/index.html#git-hooks",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Git Hooks",
    "text": "Git Hooks"
  },
  {
    "objectID": "posts/how-to-use-git/index.html#advanced-branch-management",
    "href": "posts/how-to-use-git/index.html#advanced-branch-management",
    "title": "A Guide to Git Commands: From Basic to Advanced",
    "section": "Advanced Branch Management",
    "text": "Advanced Branch Management"
  }
]